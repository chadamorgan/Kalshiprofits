name: Find Sportsbook Opportunities

on:
  # This runs the job on a schedule
  schedule:
    # Runs "at minute 0, every 6 hours" (00:00, 06:00, 12:00, 18:00 UTC)
    - cron: '0 */6 * * *'

  # This also lets you run it manually from the "Actions" tab
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1. Checks out your repository's code so the script can run
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. Sets up a Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3. Installs the Python "requests" library
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      # 4. Runs your actual scraper script
      - name: Run scraper
        env:
          # This line securely passes your API key to the script
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
        run: python find_ops.py 

      # 5. Commits the new opportunities.json file back to the repo
      - name: Commit data
        run: |
          git config --global user.name 'github-actions'
          git config --global user.email 'github-actions@github.com'
          git add opportunities.json
          # This command only commits if the file has changed
          git commit -m "Update opportunities data" || echo "No changes to commit"
          git push
